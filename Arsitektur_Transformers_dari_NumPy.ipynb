{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tugas NLP Implementasi Arsitektur Transformer\n",
        "Raden Aryo Bismo Nugroho | 22/494473/TK/54233"
      ],
      "metadata": {
        "id": "yfMG0XU25RjT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inisialisasi dan Hyperparameters"
      ],
      "metadata": {
        "id": "bkPF1fyDwa1n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "t776VNAcwVtd"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "vocab_size = 1000\n",
        "d_model = 512\n",
        "n_heads = 8\n",
        "n_layers = 6\n",
        "d_ff = 2048\n",
        "max_seq_len = 100"
      ],
      "metadata": {
        "id": "onss1ChKwiKA"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure d_model is divisible by n_heads\n",
        "assert d_model % n_heads == 0\n",
        "d_k = d_model // n_heads"
      ],
      "metadata": {
        "id": "oZ-LUBKHxpRY"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Token Embedding"
      ],
      "metadata": {
        "id": "vhBxK9L4yCo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def token_embedding(tokens, vocab_size, d_model):\n",
        "    # Create the embedding matrix (lookup table)\n",
        "    # In a real model, these weights would be learned. Here, we initialize them randomly.\n",
        "    embedding_matrix = np.random.randn(vocab_size, d_model)\n",
        "\n",
        "    # Retrieve the embeddings for the input tokens\n",
        "    return embedding_matrix[tokens]"
      ],
      "metadata": {
        "id": "-BqdFdoDyFza"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dummy input: batch of 2 sequences, each with 5 tokens\n",
        "dummy_tokens = np.array([\n",
        "    [10, 2, 45, 70, 33],\n",
        "    [5, 88, 12, 21, 9]\n",
        "])\n",
        "\n",
        "embedding_output = token_embedding(dummy_tokens, vocab_size, d_model)\n",
        "print(\"Shape of Token Embeddings:\", embedding_output.shape)\n",
        "# Expected output: (2, 5, 512)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiBCszrgzScD",
        "outputId": "545286f6-777f-4107-b205-95f123dd6ecd"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Token Embeddings: (2, 5, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encoding (Sinusoidal)"
      ],
      "metadata": {
        "id": "piufXT4SzgW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def positional_encoding(seq_len, d_model):\n",
        "\n",
        "    position = np.arange(seq_len)[:, np.newaxis]\n",
        "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
        "\n",
        "    pe = np.zeros((seq_len, d_model))\n",
        "\n",
        "    pe[:, 0::2] = np.sin(position * div_term)\n",
        "    pe[:, 1::2] = np.cos(position * div_term)\n",
        "\n",
        "    return pe"
      ],
      "metadata": {
        "id": "vo8sF3yHzizs"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the positional encoding to the token embeddings\n",
        "seq_len = dummy_tokens.shape[1]\n",
        "pos_encoding = positional_encoding(seq_len, d_model)\n",
        "x = embedding_output + pos_encoding\n",
        "\n",
        "print(\"Shape after adding Positional Encoding:\", x.shape)\n",
        "# Expected output: (2, 5, 512)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ps43zTm0r70",
        "outputId": "4aaf847c-df47-46f5-e520-d1d097cdeda8"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape after adding Positional Encoding: (2, 5, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Casual Masking"
      ],
      "metadata": {
        "id": "GQaNVQwE1CkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_causal_mask(seq_len):\n",
        "\n",
        "    # np.triu returns the upper triangle of an array. k=1 offsets it by one.\n",
        "    mask = np.triu(np.ones((seq_len, seq_len)), k=1)\n",
        "\n",
        "    # set the masked values to a very small number (negative infinity), so they become zero after the softmax operation.\n",
        "    return mask * -1e9"
      ],
      "metadata": {
        "id": "Ws3pw5Qh1EJm"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "causal_mask = create_causal_mask(seq_len)\n",
        "print(\"Causal Mask (for seq_len=5):\")\n",
        "print(causal_mask)\n",
        "# Expected output: A 5x5 matrix with 0s on and below the diagonal, and -1e9 above it."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdyLiBYo1qJL",
        "outputId": "4015c8bc-08d6-4420-c689-db8d1dad352d"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Causal Mask (for seq_len=5):\n",
            "[[-0.e+00 -1.e+09 -1.e+09 -1.e+09 -1.e+09]\n",
            " [-0.e+00 -0.e+00 -1.e+09 -1.e+09 -1.e+09]\n",
            " [-0.e+00 -0.e+00 -0.e+00 -1.e+09 -1.e+09]\n",
            " [-0.e+00 -0.e+00 -0.e+00 -0.e+00 -1.e+09]\n",
            " [-0.e+00 -0.e+00 -0.e+00 -0.e+00 -0.e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaled Dot-Product Attention"
      ],
      "metadata": {
        "id": "wlxhyQb917P1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    # Computes softmax along the last axis, with numerical stability\n",
        "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return e_x / np.sum(e_x, axis=-1, keepdims=True)"
      ],
      "metadata": {
        "id": "m5ZRfvHV1_UU"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "\n",
        "    # Matrix multiplication Q and K.T\n",
        "    # Q: (..., seq_len, d_k), K.T: (..., d_k, seq_len) -> scores: (..., seq_len, seq_len)\n",
        "    scores = np.matmul(Q, K.swapaxes(-2, -1))\n",
        "\n",
        "    # Scale the scores\n",
        "    d_k = Q.shape[-1]\n",
        "    scaled_scores = scores / np.sqrt(d_k)\n",
        "\n",
        "    # Apply mask if provided\n",
        "    if mask is not None:\n",
        "        scaled_scores += mask\n",
        "\n",
        "    # Get attention weights using softmax\n",
        "    attention_weights = softmax(scaled_scores)\n",
        "\n",
        "    # Matrix multiplication weights and V\n",
        "    # weights: (..., seq_len, seq_len), V: (..., seq_len, d_k) -> output: (..., seq_len, d_k)\n",
        "    output = np.matmul(attention_weights, V)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "metadata": {
        "id": "wZRvFHiG2Is1"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Head Attention"
      ],
      "metadata": {
        "id": "UkdQMn0k25wD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention:\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads\n",
        "\n",
        "        # Initialize weights for Q, K, V projections and the final output projection\n",
        "        self.W_q = np.random.randn(d_model, d_model)\n",
        "        self.W_k = np.random.randn(d_model, d_model)\n",
        "        self.W_v = np.random.randn(d_model, d_model)\n",
        "        self.W_o = np.random.randn(d_model, d_model)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # x shape: (batch_size, seq_len, d_model)\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        # Reshape to (batch_size, seq_len, n_heads, d_k)\n",
        "        x = x.reshape(batch_size, seq_len, self.n_heads, self.d_k)\n",
        "        # Transpose to (batch_size, n_heads, seq_len, d_k) for attention calculation\n",
        "        return x.transpose(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # x shape: (batch_size, seq_len, d_model)\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        # 1. Project to Q, K, V\n",
        "        Q = x @ self.W_q\n",
        "        K = x @ self.W_k\n",
        "        V = x @ self.W_v\n",
        "\n",
        "        # 2. Split into multiple heads\n",
        "        Q = self.split_heads(Q)\n",
        "        K = self.split_heads(K)\n",
        "        V = self.split_heads(V)\n",
        "\n",
        "        # 3. Apply scaled dot-product attention\n",
        "        # Q, K, V shapes: (batch_size, n_heads, seq_len, d_k)\n",
        "        attention_output, attention_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "        # print(\"Bukti Causal Masking (Attention Weights)\")\n",
        "        # print(attention_weights[0, 0, :, :])\n",
        "\n",
        "        # 4. Concatenate heads\n",
        "        # Transpose back to (batch_size, seq_len, n_heads, d_k)\n",
        "        attention_output = attention_output.transpose(0, 2, 1, 3)\n",
        "        # Reshape to (batch_size, seq_len, d_model)\n",
        "        concatenated_output = attention_output.reshape(batch_size, seq_len, self.d_model)\n",
        "\n",
        "        # 5. Final linear projection\n",
        "        output = concatenated_output @ self.W_o\n",
        "        return output"
      ],
      "metadata": {
        "id": "81SdvQ8E28yD"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mha = MultiHeadAttention(d_model, n_heads)\n",
        "# 'x' is our data after adding positional encoding\n",
        "# We pass the same 'x' for Q, K, and V in self-attention\n",
        "mha_output = mha.forward(x, causal_mask)\n",
        "\n",
        "print(\"Shape of Multi-Head Attention output:\", mha_output.shape)\n",
        "# Expected output: (2, 5, 512)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQ1YjQMm3C2T",
        "outputId": "72bf1d54-6bbc-4724-c8c6-82d033d8cf2c"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bukti Causal Masking (Attention Weights)\n",
            "[[1.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
            "  0.00000000e+000]\n",
            " [2.99041815e-058 1.00000000e+000 0.00000000e+000 0.00000000e+000\n",
            "  0.00000000e+000]\n",
            " [5.50377254e-154 0.00000000e+000 1.00000000e+000 0.00000000e+000\n",
            "  0.00000000e+000]\n",
            " [0.00000000e+000 0.00000000e+000 0.00000000e+000 1.00000000e+000\n",
            "  0.00000000e+000]\n",
            " [6.52441429e-226 0.00000000e+000 0.00000000e+000 1.07800040e-214\n",
            "  1.00000000e+000]]\n",
            "Shape of Multi-Head Attention output: (2, 5, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layer Normalization dan Feed-Forward Network"
      ],
      "metadata": {
        "id": "vo1EugLk3azn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization:\n",
        "    def __init__(self, d_model, epsilon=1e-5):\n",
        "        self.epsilon = epsilon\n",
        "        # Gamma and Beta are learnable parameters\n",
        "        self.gamma = np.ones(d_model)\n",
        "        self.beta = np.zeros(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, d_model)\n",
        "        mean = np.mean(x, axis=-1, keepdims=True)\n",
        "        std = np.std(x, axis=-1, keepdims=True)\n",
        "\n",
        "        # Normalize\n",
        "        normalized_x = (x - mean) / (std + self.epsilon)\n",
        "\n",
        "        # Scale and shift\n",
        "        return self.gamma * normalized_x + self.beta"
      ],
      "metadata": {
        "id": "XXZSc9rU3c0O"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNetwork:\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        # Initialize weights and biases\n",
        "        self.W1 = np.random.randn(d_model, d_ff)\n",
        "        self.b1 = np.zeros(d_ff)\n",
        "        self.W2 = np.random.randn(d_ff, d_model)\n",
        "        self.b2 = np.zeros(d_model)\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First linear layer + ReLU\n",
        "        hidden = self.relu((x @ self.W1) + self.b1)\n",
        "        # Second linear layer\n",
        "        output = (hidden @ self.W2) + self.b2\n",
        "        return output"
      ],
      "metadata": {
        "id": "RBNSIzNB3q1Q"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ffn = FeedForwardNetwork(d_model, d_ff)\n",
        "ln1 = LayerNormalization(d_model)\n",
        "ln2 = LayerNormalization(d_model)"
      ],
      "metadata": {
        "id": "A5YL3Dga4Zt1"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-Norm & Residual Connection"
      ],
      "metadata": {
        "id": "AD-6fY7G4Oe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the \"pre-norm\" flow for one block\n",
        "# First sublayer: Multi-Head Attention\n",
        "norm_x = ln1.forward(x)\n",
        "mha_output = mha.forward(norm_x, causal_mask)\n",
        "# Residual connection\n",
        "add_mha = x + mha_output\n",
        "\n",
        "# Second sublayer: Feed-Forward Network\n",
        "norm_add_mha = ln2.forward(add_mha)\n",
        "ffn_output = ffn.forward(norm_add_mha)\n",
        "# Residual connection\n",
        "add_ffn = add_mha + ffn_output\n",
        "\n",
        "print(\"Shape after one Transformer block:\", add_ffn.shape)\n",
        "# Expected output: (2, 5, 512)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "372RAruF3uAg",
        "outputId": "4ffd6ffc-dbd4-4d80-d89e-900254ccc93a"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bukti Causal Masking (Attention Weights)\n",
            "[[1.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
            "  0.00000000e+000]\n",
            " [7.13835174e-167 1.00000000e+000 0.00000000e+000 0.00000000e+000\n",
            "  0.00000000e+000]\n",
            " [4.29528361e-273 0.00000000e+000 1.00000000e+000 0.00000000e+000\n",
            "  0.00000000e+000]\n",
            " [3.41595332e-313 4.87664770e-165 8.96140256e-243 1.00000000e+000\n",
            "  0.00000000e+000]\n",
            " [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
            "  1.00000000e+000]]\n",
            "Shape after one Transformer block: (2, 5, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Complete Tranformer Model"
      ],
      "metadata": {
        "id": "fEY7OhnM4ti3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock:\n",
        "    def __init__(self, d_model, n_heads, d_ff):\n",
        "        self.attention = MultiHeadAttention(d_model, n_heads)\n",
        "        self.ffn = FeedForwardNetwork(d_model, d_ff)\n",
        "        self.ln1 = LayerNormalization(d_model)\n",
        "        self.ln2 = LayerNormalization(d_model)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # Pre-norm attention sublayer\n",
        "        norm_x = self.ln1.forward(x)\n",
        "        attention_output = self.attention.forward(norm_x, mask)\n",
        "        x = x + attention_output # Residual connection\n",
        "\n",
        "        # Pre-norm feed-forward sublayer\n",
        "        norm_x = self.ln2.forward(x)\n",
        "        ffn_output = self.ffn.forward(norm_x)\n",
        "        x = x + ffn_output # Residual connection\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "zSXwXYUl4w8f"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderOnlyTransformer:\n",
        "    def __init__(self, vocab_size, d_model, n_layers, n_heads, d_ff, max_seq_len):\n",
        "        self.embedding_matrix = np.random.randn(vocab_size, d_model)\n",
        "        self.pos_encoding_matrix = positional_encoding(max_seq_len, d_model)\n",
        "\n",
        "        self.blocks = [TransformerBlock(d_model, n_heads, d_ff) for _ in range(n_layers)]\n",
        "\n",
        "        self.final_ln = LayerNormalization(d_model)\n",
        "\n",
        "        # The final linear layer to project to vocab size\n",
        "        self.output_projection = np.random.randn(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        batch_size, seq_len = tokens.shape\n",
        "\n",
        "        # 1. Create mask\n",
        "        mask = create_causal_mask(seq_len)\n",
        "\n",
        "        # 2. Input embedding and positional encoding\n",
        "        x = self.embedding_matrix[tokens]\n",
        "        x += self.pos_encoding_matrix[:seq_len, :]\n",
        "\n",
        "        # 3. Pass through Transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block.forward(x, mask)\n",
        "\n",
        "        # 4. Final normalization\n",
        "        x = self.final_ln.forward(x)\n",
        "\n",
        "        # 5. Project to vocabulary space\n",
        "        # Logits are the raw scores before softmax\n",
        "        logits = x @ self.output_projection\n",
        "\n",
        "        # 6. Get probability distribution\n",
        "        probs = softmax(logits)\n",
        "\n",
        "        return logits, probs"
      ],
      "metadata": {
        "id": "UD-7LO_z47QI"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FInal Test\n",
        "\n",
        "# Instantiate the full model\n",
        "model = DecoderOnlyTransformer(vocab_size, d_model, n_layers, n_heads, d_ff, max_seq_len)\n",
        "\n",
        "# Run the forward pass with our dummy tokens\n",
        "final_logits, final_probs = model.forward(dummy_tokens)\n",
        "\n",
        "print(\"\\n--- Final Output ---\")\n",
        "print(\"Shape of Logits:\", final_logits.shape)\n",
        "# Expected output: (2, 5, 1000)\n",
        "print(\"Shape of Probabilities:\", final_probs.shape)\n",
        "# Expected output: (2, 5, 1000)\n",
        "\n",
        "# To predict the NEXT token, we only care about the last position in the sequence\n",
        "next_token_probs = final_probs[:, -1, :]\n",
        "print(\"\\nProbabilities for the next token (shape):\", next_token_probs.shape)\n",
        "# Expected output: (2, 1000)\n",
        "\n",
        "# Check if probabilities sum to 1 for each sequence in the batch\n",
        "print(\"Sum of probabilities for next token (should be ~1.0):\", np.sum(next_token_probs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkDXR2_M4-b4",
        "outputId": "8270f931-ff9f-463f-8a59-9e7ca731e4ab"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bukti Causal Masking (Attention Weights)\n",
            "[[1.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
            "  0.00000000e+000]\n",
            " [1.00000000e+000 9.54312342e-016 0.00000000e+000 0.00000000e+000\n",
            "  0.00000000e+000]\n",
            " [0.00000000e+000 1.00000000e+000 7.52482974e-176 0.00000000e+000\n",
            "  0.00000000e+000]\n",
            " [0.00000000e+000 0.00000000e+000 1.00000000e+000 0.00000000e+000\n",
            "  0.00000000e+000]\n",
            " [0.00000000e+000 1.00000000e+000 4.98354934e-077 1.38915299e-172\n",
            "  0.00000000e+000]]\n",
            "Bukti Causal Masking (Attention Weights)\n",
            "[[1.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
            "  0.00000000e+000]\n",
            " [4.51447463e-017 1.00000000e+000 0.00000000e+000 0.00000000e+000\n",
            "  0.00000000e+000]\n",
            " [0.00000000e+000 0.00000000e+000 1.00000000e+000 0.00000000e+000\n",
            "  0.00000000e+000]\n",
            " [0.00000000e+000 0.00000000e+000 1.00000000e+000 0.00000000e+000\n",
            "  0.00000000e+000]\n",
            " [3.38736114e-258 7.64835387e-193 1.00000000e+000 3.89727748e-125\n",
            "  3.94291144e-217]]\n",
            "Bukti Causal Masking (Attention Weights)\n",
            "[[1.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
            "  0.00000000e+000]\n",
            " [1.76790230e-038 1.00000000e+000 0.00000000e+000 0.00000000e+000\n",
            "  0.00000000e+000]\n",
            " [3.01415055e-182 8.35001698e-185 1.00000000e+000 0.00000000e+000\n",
            "  0.00000000e+000]\n",
            " [1.89445271e-178 6.52956447e-262 1.00000000e+000 5.63980208e-069\n",
            "  0.00000000e+000]\n",
            " [1.87004830e-071 3.15156874e-183 1.00000000e+000 9.15194258e-058\n",
            "  1.79318418e-183]]\n",
            "Bukti Causal Masking (Attention Weights)\n",
            "[[1.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
            "  0.00000000e+000]\n",
            " [1.00000000e+000 1.27175537e-138 0.00000000e+000 0.00000000e+000\n",
            "  0.00000000e+000]\n",
            " [5.68848435e-205 1.00718582e-255 1.00000000e+000 0.00000000e+000\n",
            "  0.00000000e+000]\n",
            " [1.54642547e-321 0.00000000e+000 1.00000000e+000 0.00000000e+000\n",
            "  0.00000000e+000]\n",
            " [1.00794931e-103 3.51199532e-231 1.00000000e+000 5.57314764e-184\n",
            "  9.05731833e-175]]\n",
            "Bukti Causal Masking (Attention Weights)\n",
            "[[1.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
            "  0.00000000e+000]\n",
            " [1.00000000e+000 8.48217640e-147 0.00000000e+000 0.00000000e+000\n",
            "  0.00000000e+000]\n",
            " [1.00000000e+000 4.24948434e-091 3.24426389e-135 0.00000000e+000\n",
            "  0.00000000e+000]\n",
            " [2.39884846e-134 4.72410908e-302 5.52666637e-315 1.00000000e+000\n",
            "  0.00000000e+000]\n",
            " [1.70730779e-061 1.27363159e-068 2.38802503e-183 1.00000000e+000\n",
            "  2.28328020e-010]]\n",
            "Bukti Causal Masking (Attention Weights)\n",
            "[[1.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
            "  0.00000000e+000]\n",
            " [1.33229738e-144 1.00000000e+000 0.00000000e+000 0.00000000e+000\n",
            "  0.00000000e+000]\n",
            " [9.74065381e-144 1.00000000e+000 6.31839767e-100 0.00000000e+000\n",
            "  0.00000000e+000]\n",
            " [2.81397772e-227 1.00000000e+000 1.68468735e-030 1.96485958e-226\n",
            "  0.00000000e+000]\n",
            " [8.25343636e-233 1.33749553e-090 1.00000000e+000 5.68062253e-147\n",
            "  1.34089219e-092]]\n",
            "\n",
            "--- Final Output ---\n",
            "Shape of Logits: (2, 5, 1000)\n",
            "Shape of Probabilities: (2, 5, 1000)\n",
            "\n",
            "Probabilities for the next token (shape): (2, 1000)\n",
            "Sum of probabilities for next token (should be ~1.0): 1.0000000000000002\n"
          ]
        }
      ]
    }
  ]
}